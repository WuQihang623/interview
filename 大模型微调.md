### 介绍LoRA与QLoRA



### 为什么会有LLM的偏见



### 什么是思维链（COT）提示



### Tokenizer的实现方法与原理



### 解释一下大模型的涌向能力



### langchanAgent



### RLHF完整训练过程是什么



### RAG和微调的区别



### 稀疏微调



### FlashAttention的原理



### LLM预训练阶段有哪些关键步骤



### RLHF模型为什么比SFT更好



### LLaMA为什么要用旋转位置编码



### DeepSpeed推理对算子融合做了什么优化



### MHA，GQA，MQA三种注意力机制的区别是什么



### 大模型为什么多数是decoder-only的



### 训练后量化（PTQ）和量化感知训练（QAT）的区别



